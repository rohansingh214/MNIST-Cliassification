{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle #import all essentials\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb') #unpack zip\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "\"\"\"Since the data loaded is in tuples, we split the data into \n",
    "data points and target points\"\"\"\n",
    "training_matrix = training_data[0] \n",
    "training_target = training_data[1]\n",
    "validation_matrix = validation_data[0]\n",
    "validation_target = validation_data[1]\n",
    "test_matrix = test_data[0]\n",
    "test_target = test_data[1]\n",
    "print(training_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10): #preparing USPS data\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28)) #resize\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255 #image features\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)\n",
    "USPSMat = np.asarray(USPSMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(h, y): #loss function for Logistic\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "def softmax(z):\n",
    "    ma = np.max(z, axis=1).reshape((-1,1))\n",
    "    ex = np.exp(z-ma)\n",
    "    den = np.sum(ex, axis=1).reshape((-1,1))\n",
    "    return ex/den #softmax function exp(a)/sum(exp(a))\n",
    "\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)]) #onehot encoding converts a sample vector to samples x number_of_class\n",
    "\n",
    "def logistic_regression(X, y):\n",
    "    target = one_hot(y, 10) #convert target to one hot\n",
    "    iterations = 4000\n",
    "    learning_rate = 0.04\n",
    "    weights = np.zeros([X.shape[1], 10]) #empty weight matrix\n",
    "    for _ in range(iterations):\n",
    "        for i in range(0, 50000, 100):\n",
    "            x = np.dot(X[i:i+100], weights) #training batch-wise with 100 samples at time\n",
    "            h = softmax(x) #applying softmax\n",
    "            gradient_des = np.array(np.dot(X[i:i+100].T, (h - target[i:i+100])) / target[i:i+100].shape[0]) #calculate gradient descent\n",
    "            weights -= learning_rate * gradient_des #update weights\n",
    "    return weights\n",
    "\n",
    "def eval_model(test_matrix, test_target, weights): #calculate accuracy\n",
    "    ctr = 0\n",
    "    c = np.dot(test_matrix, weights)\n",
    "    ycap = softmax(c) #10000 x 10\n",
    "    y_cap = np.argmax(ycap, axis=1) #converts one hot to predicted target vector\n",
    "    for i in range(y_cap.shape[0]):\n",
    "        if(y_cap[i] == test_target[i]):\n",
    "            ctr += 1\n",
    "    acc = (ctr/test_matrix.shape[0]) * 100 #calculate accuracy\n",
    "    actual = pd.Series(test_target, name='actual')\n",
    "    predicted = pd.Series(y_cap, name='predicted')\n",
    "    print(pd.crosstab(actual, predicted)) #confusion matrix\n",
    "    return acc, y_cap\n",
    "    \n",
    "\n",
    "weights = logistic_regression(training_matrix, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted    0     1    2    3    4    5    6    7    8    9\n",
      "actual                                                      \n",
      "0          953     0    1    4    1    5    8    3    5    0\n",
      "1            0  1115    3    3    0    1    3    2    8    0\n",
      "2            5    12  914   20    7    8   11   10   43    2\n",
      "3            4     2   14  928    3   20    3   10   19    7\n",
      "4            2     1    7    4  918    0    7    5    9   29\n",
      "5            9     4    4   43    9  759   17    8   33    6\n",
      "6            9     3    6    4    5   17  910    1    3    0\n",
      "7            2     7   19   12    5    1    0  945    5   32\n",
      "8            7    13    7   24    7   26   10   13  856   11\n",
      "9            8     7    1   11   25    8    0   25   11  913\n",
      "\n",
      "\n",
      "Accuracy - Logistic Regression on MNIST test data = 92.11\n",
      "\n",
      "\n",
      "predicted    0     1    2    3    4    5    6     7    8    9\n",
      "actual                                                       \n",
      "0          957     0    5    2    2    7    7     4    6    1\n",
      "1            0  1042    2    6    1    3    0     2    7    1\n",
      "2            5     9  898   21   10    2    5    15   21    4\n",
      "3            6     0   12  938    1   39    3     3   18   10\n",
      "4            4     8    2    3  927    0   11     4    3   21\n",
      "5           14     3    9   36    5  790   26     5   21    6\n",
      "6            4     3    9    0    5    6  936     1    3    0\n",
      "7            9     3    7    8    7    0    0  1023    1   32\n",
      "8            2    17    7   24    2   24    5     9  903   16\n",
      "9            5     5    2   10   27    4    1    41   10  856\n",
      "\n",
      "\n",
      "Accuracy - Logistic Regression on MNIST validation data = (92.7, array([3, 8, 6, ..., 5, 6, 8], dtype=int64))\n",
      "\n",
      "\n",
      "predicted    0    1     2    3    4    5    6    7    8    9\n",
      "actual                                                      \n",
      "0          298    0   303  255  107  340   83  380   86  148\n",
      "1           51  197   509  152  267  182   31  378  188   45\n",
      "2           76   34  1218  127   39  278  104   33   71   19\n",
      "3           37   32   399  690   23  663   14   67   53   22\n",
      "4           24   17   137   84  683  192   53  507  211   92\n",
      "5           34   16   631  178   45  908   59   62   55   12\n",
      "6          118    3   749   99   44  438  477   34   15   23\n",
      "7           98   55   100  609  107  136   15  622  212   46\n",
      "8          161   20   217  519  112  550   84  146  155   36\n",
      "9           18   26   125  574  116  103   11  653  214  160\n",
      "\n",
      "\n",
      "Accuracy - Logistic Regression on USPS data = (27.04135206760338, array([5, 7, 5, ..., 7, 3, 3], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "acc, LR_predicted = eval_model(test_matrix, test_target, weights)\n",
    "print(\"Accuracy - Logistic Regression on MNIST test data = \"+str(acc))\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy - Logistic Regression on MNIST validation data = \"+str(eval_model(validation_matrix, validation_target, weights)))\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy - Logistic Regression on USPS data = \"+str(eval_model(np.asarray(USPSMat), np.asarray(USPSTar), weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(first_layer_nodes, second_layer_nodes, third_layer_nodes, input_size, drop_out):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_layer_nodes, input_dim=input_size)) #first layer nodes\n",
    "    model.add(Activation('relu')) #activation relu\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(second_layer_nodes))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(third_layer_nodes))\n",
    "    model.add(Activation('softmax')) #output layer with activation softmax since there are multiple classes to predict\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "\n",
    "def _nn(data, target, test_data, test_target, test_data1, test_target1, test_data2, test_target2):\n",
    "    input_size = data.shape[1]\n",
    "    drop_out = 0.2\n",
    "    first_layer_nodes = 512\n",
    "    second_layer_nodes = 512\n",
    "    third_layer_nodes = 10\n",
    "    model = get_model(first_layer_nodes, second_layer_nodes, third_layer_nodes, input_size, drop_out)\n",
    "    validation_data_split = 0.2\n",
    "    num_epochs = 50 #50 num of epochs\n",
    "    model_batch_size = 128 \n",
    "    tb_batch_size = 32\n",
    "    early_patience = 500\n",
    "    \n",
    "    tensorboard_cb = TensorBoard(log_dir='logs', batch_size=tb_batch_size, write_graph=True)\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "    \n",
    "    history = model.fit(data, #training \n",
    "                       target,\n",
    "                       validation_split=validation_data_split,\n",
    "                       epochs=num_epochs,\n",
    "                       batch_size=model_batch_size,\n",
    "                       callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                       )\n",
    "#     %matplotlib inline\n",
    "#     df = pd.DataFrame(history.history)\n",
    "#     df.plot(subplots=True, grid=True, figsize=(10, 15))\n",
    "    NN_predicted = []\n",
    "    yy = model.predict(np.array(test_data)) #output predicted labels\n",
    "    #print(yy.shape)\n",
    "    for i in yy:\n",
    "        NN_predicted.append(np.argmax(i))\n",
    "    #print(NN_predicted)\n",
    "    \n",
    "    ne1, score1 = model.evaluate(test_data, test_target, batch_size=128) #evaluating the accuracy for test data\n",
    "    ne2, score2 = model.evaluate(test_data1, test_target1, batch_size=128)\n",
    "    ne3, score3 = model.evaluate(test_data2, test_target2, batch_size=128)\n",
    "    print(\"Accuracy for MNIST test Data = \"+str(score1*100))\n",
    "    print(\"Accuracy for MNIST validation Data = \"+str(score3*100))\n",
    "    print(\"Accuracy for USPS Data = \"+str(score2*100))\n",
    "    \n",
    "    return NN_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 6s 149us/step - loss: 0.3033 - acc: 0.9090 - val_loss: 0.1458 - val_acc: 0.9554\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: 0.1179 - acc: 0.9634 - val_loss: 0.1141 - val_acc: 0.9657\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.0785 - acc: 0.9750 - val_loss: 0.1166 - val_acc: 0.9638\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0591 - acc: 0.9811 - val_loss: 0.1086 - val_acc: 0.9686\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 0.0481 - acc: 0.9843 - val_loss: 0.0990 - val_acc: 0.9712\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 0.0391 - acc: 0.9877 - val_loss: 0.1033 - val_acc: 0.9721\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 0.0333 - acc: 0.9892 - val_loss: 0.1053 - val_acc: 0.9722\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0317 - acc: 0.9899 - val_loss: 0.1026 - val_acc: 0.9721\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 0.0236 - acc: 0.9919 - val_loss: 0.1072 - val_acc: 0.9728\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0242 - acc: 0.9918 - val_loss: 0.1216 - val_acc: 0.9708\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: 0.0237 - acc: 0.9918 - val_loss: 0.1206 - val_acc: 0.9729\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0210 - acc: 0.9931 - val_loss: 0.1154 - val_acc: 0.9732\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 6s 138us/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.1130 - val_acc: 0.9751\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 6s 141us/step - loss: 0.0180 - acc: 0.9936 - val_loss: 0.1178 - val_acc: 0.9756\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0164 - acc: 0.9944 - val_loss: 0.1214 - val_acc: 0.9757\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: 0.0178 - acc: 0.9941 - val_loss: 0.1238 - val_acc: 0.9731\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 6s 139us/step - loss: 0.0188 - acc: 0.9932 - val_loss: 0.1147 - val_acc: 0.9759\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 6s 142us/step - loss: 0.0162 - acc: 0.9946 - val_loss: 0.1164 - val_acc: 0.9781\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0148 - acc: 0.9951 - val_loss: 0.1129 - val_acc: 0.9762\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 6s 142us/step - loss: 0.0128 - acc: 0.9954 - val_loss: 0.1374 - val_acc: 0.9727\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.1353 - val_acc: 0.9751\n",
      "Epoch 22/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0139 - acc: 0.9952 - val_loss: 0.1301 - val_acc: 0.9749\n",
      "Epoch 23/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.1252 - val_acc: 0.9768\n",
      "Epoch 24/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0122 - acc: 0.9958 - val_loss: 0.1535 - val_acc: 0.9711\n",
      "Epoch 25/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0144 - acc: 0.9953 - val_loss: 0.1162 - val_acc: 0.9775\n",
      "Epoch 26/50\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.1332 - val_acc: 0.9738\n",
      "Epoch 27/50\n",
      "40000/40000 [==============================] - 6s 138us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.1255 - val_acc: 0.9755\n",
      "Epoch 28/50\n",
      "40000/40000 [==============================] - 6s 154us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.1171 - val_acc: 0.9770\n",
      "Epoch 29/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.1303 - val_acc: 0.9766\n",
      "Epoch 30/50\n",
      "40000/40000 [==============================] - 6s 139us/step - loss: 0.0133 - acc: 0.9956 - val_loss: 0.1534 - val_acc: 0.9746\n",
      "Epoch 31/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.1461 - val_acc: 0.9754\n",
      "Epoch 32/50\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.1317 - val_acc: 0.9763\n",
      "Epoch 33/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0087 - acc: 0.9973 - val_loss: 0.1433 - val_acc: 0.9757\n",
      "Epoch 34/50\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: 0.0100 - acc: 0.9966 - val_loss: 0.1339 - val_acc: 0.9769\n",
      "Epoch 35/50\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.1380 - val_acc: 0.9769\n",
      "Epoch 36/50\n",
      "40000/40000 [==============================] - 6s 147us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.1612 - val_acc: 0.9735\n",
      "Epoch 37/50\n",
      "40000/40000 [==============================] - 6s 146us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.1487 - val_acc: 0.9766\n",
      "Epoch 38/50\n",
      "40000/40000 [==============================] - 6s 149us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.1393 - val_acc: 0.9764\n",
      "Epoch 39/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.1582 - val_acc: 0.9759\n",
      "Epoch 40/50\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.1624 - val_acc: 0.9741\n",
      "Epoch 41/50\n",
      "40000/40000 [==============================] - 6s 144us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 0.1584 - val_acc: 0.9758\n",
      "Epoch 42/50\n",
      "40000/40000 [==============================] - 6s 148us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.1570 - val_acc: 0.9753\n",
      "Epoch 43/50\n",
      "40000/40000 [==============================] - 6s 143us/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.1561 - val_acc: 0.9770\n",
      "Epoch 44/50\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.1462 - val_acc: 0.9774\n",
      "Epoch 45/50\n",
      "40000/40000 [==============================] - 6s 149us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 0.1544 - val_acc: 0.9760\n",
      "Epoch 46/50\n",
      "40000/40000 [==============================] - 6s 142us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.1588 - val_acc: 0.9769\n",
      "Epoch 47/50\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.1433 - val_acc: 0.9774\n",
      "Epoch 48/50\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.1518 - val_acc: 0.9782\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 133us/step - loss: 0.0096 - acc: 0.9972 - val_loss: 0.1403 - val_acc: 0.9779\n",
      "Epoch 50/50\n",
      "40000/40000 [==============================] - 5s 137us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.1351 - val_acc: 0.9789\n",
      "10000/10000 [==============================] - 0s 30us/step\n",
      "19999/19999 [==============================] - 1s 33us/step\n",
      "10000/10000 [==============================] - 0s 29us/step\n",
      "Accuracy for MNIST test Data = 98.22\n",
      "Accuracy for MNIST validation Data = 98.18\n",
      "Accuracy for USPS Data = 48.27741387024648\n",
      "predicted    0     1     2    3    4    5    6    7    8    9\n",
      "actual                                                       \n",
      "0          970     0     2    1    0    1    2    1    1    2\n",
      "1            0  1120     3    3    0    1    1    3    4    0\n",
      "2            0     0  1022    3    1    0    1    2    3    0\n",
      "3            0     0     2  995    0    2    0    3    4    4\n",
      "4            1     0     4    0  961    0    3    2    2    9\n",
      "5            1     0     0   12    0  871    2    0    2    4\n",
      "6            3     2     1    1    3    4  942    0    2    0\n",
      "7            0     0    15    2    0    0    0  995    4   12\n",
      "8            1     0     6    4    2    1    0    2  955    3\n",
      "9            0     2     0    2    7    4    0    3    0  991\n"
     ]
    }
   ],
   "source": [
    "NN_predicted = _nn(training_matrix, \n",
    "                   one_hot(training_target, 10), \n",
    "                   test_matrix, \n",
    "                   one_hot(test_target, 10), \n",
    "                   np.asarray(USPSMat), \n",
    "                   one_hot(np.asarray(USPSTar), 10), \n",
    "                   validation_matrix, \n",
    "                   one_hot(validation_target, 10))\n",
    "actual = pd.Series(test_target, name='actual')\n",
    "predicted = pd.Series(NN_predicted, name='predicted')\n",
    "print(pd.crosstab(actual, predicted)) #confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST test data = 93.89999999999999\n",
      "Accuracy for MNIST validation data = 94.23\n",
      "Accuracy for USPS data = 29.12645632281614\n"
     ]
    }
   ],
   "source": [
    "#Linear SVM\n",
    "def linear_classification_default_gamma(data, target, test_matrix1, test_target1, \n",
    "                                        test_matrix2, test_target2, \n",
    "                                        test_matrix3, test_target3):\n",
    "    classification = svm.SVC(kernel='linear', gamma='auto') #gamma set to default with linear kernel\n",
    "    classification.fit(data, target)\n",
    "    predicted_val1 = classification.predict(test_matrix1) #predict op labels\n",
    "    predicted_val2 = classification.predict(test_matrix2)\n",
    "    predicted_val3 = classification.predict(test_matrix3)\n",
    "    ctr1, ctr2, ctr3 = 0, 0, 0\n",
    "    for i in range(len(test_target1)): #calculate accuracy\n",
    "        if(predicted_val1[i] == test_target1[i]):\n",
    "            ctr1 += 1\n",
    "        if(predicted_val2[i] == test_target2[i]):\n",
    "            ctr2 += 1\n",
    "    for i in range(len(test_target3)):\n",
    "        if(predicted_val3[i] == test_target3[i]):\n",
    "            ctr3 += 1\n",
    "    print(\"Accuracy for MNIST test data = \"+str((ctr1/len(test_target1))*100))\n",
    "    actual1 = pd.Series(test_target1, name='tt1')\n",
    "    pred1 = pd.Series(predicted_val1, name='pd1')\n",
    "    print(\"Confusion Matrix for test data\")\n",
    "    print(pd.crosstab(actual1, pred1)) #confusion matrix\n",
    "    print(\"Accuracy for MNIST validation data = \"+str((ctr2/len(test_target2))*100))\n",
    "    actual2 = pd.Series(test_target2, name='tt2')\n",
    "    pred2 = pd.Series(predicted_val2, name='pd2')\n",
    "    print(\"Confusion Matrix for validation data\")\n",
    "    print(pd.crosstab(actual2, pred2))\n",
    "    print(\"Accuracy for USPS data = \"+str((ctr3/len(test_target3))*100))\n",
    "    actual3 = pd.Series(test_target3, name='tt3')\n",
    "    pred3 = pd.Series(predicted_val3, name='pd3')\n",
    "    print(\"Confusion Matrix for USPS data\")\n",
    "    print(pd.crosstab(actual3, pred3))\n",
    "    \n",
    "linear_classification_default_gamma(training_matrix, training_target, \n",
    "                                    test_matrix, test_target, \n",
    "                                    validation_matrix, validation_target, \n",
    "                                    np.asarray(USPSMat), np.asarray(USPSTar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST test data on svm rbf gamma(default) = 94.35\n",
      "Confusion Matrix for test data\n",
      "pd1    0     1    2    3    4    5    6    7    8    9\n",
      "tt1                                                   \n",
      "0    967     0    1    0    0    5    4    1    2    0\n",
      "1      0  1120    2    3    0    1    3    1    5    0\n",
      "2      9     1  962    7   10    1   13   11   16    2\n",
      "3      1     1   14  950    1   17    1   10   11    4\n",
      "4      1     1    7    0  937    0    7    2    2   25\n",
      "5      7     4    5   33    7  808   11    2   10    5\n",
      "6     10     3    4    1    5   10  924    0    1    0\n",
      "7      2    13   22    5    7    1    0  954    4   20\n",
      "8      4     6    6   14    8   24   10    8  891    3\n",
      "9     10     6    0   12   33    5    1   14    6  922\n",
      "Accuracy for MNIST validation data on svm rbf gamma(default) = 94.48\n",
      "Confusion Matrix for validation data\n",
      "pd2    0     1    2    3    4    5    6     7    8    9\n",
      "tt2                                                    \n",
      "0    972     0    3    2    1    0    8     1    3    1\n",
      "1      0  1049    3    2    1    2    0     1    4    2\n",
      "2      5     3  929    5   10    3   12     6   11    6\n",
      "3      1     3   13  949    1   34    3     3   17    6\n",
      "4      1     7    4    0  944    0    3     2    3   19\n",
      "5      5     5    8   27    6  830   20     2   11    1\n",
      "6      2     0    5    0    3    4  950     0    3    0\n",
      "7      2    10   13    3   11    0    0  1034    1   16\n",
      "8      3    25    9   16    4   20    4     6  911   11\n",
      "9      5     6    4   14   21    4    0    21    6  880\n",
      "Accuracy for USPS data on svm rbf gamma(default) = 38.54192709635482\n",
      "Confusion Matrix for USPS data\n",
      "pd3    0    1     2     3     4     5    6    7    8    9\n",
      "tt3                                                      \n",
      "0    573    2   428    19   285   248   73   44    6  322\n",
      "1    110  429   285   137   273   180   46  501   22   17\n",
      "2    128   18  1402    59    39   198   61   57   23   14\n",
      "3     76    3   186  1123    11   483    5   70   27   16\n",
      "4     18   67    91    14  1167   267   22  194   69   91\n",
      "5    108   17   257   102    25  1367   60   43   15    6\n",
      "6    197    7   489    24    98   394  748   13    7   23\n",
      "7     50  225   457   265    57   416   15  452   41   22\n",
      "8     73   25   209   193    87  1006   95   41  244   27\n",
      "9     26  166   228   278   213   165    8  499  214  203\n"
     ]
    }
   ],
   "source": [
    "#RBF SVM with gamma default\n",
    "def rbf_classification_default_gamma(data, target, test_matrix1, test_target1, \n",
    "                                        test_matrix2, test_target2, \n",
    "                                        test_matrix3, test_target3):\n",
    "    classification = svm.SVC(kernel='rbf', gamma='auto') #gamma set to 1 with rbf kernel\n",
    "    classification.fit(data, target)\n",
    "    predicted_val1 = classification.predict(test_matrix1)\n",
    "    predicted_val2 = classification.predict(test_matrix2)\n",
    "    predicted_val3 = classification.predict(test_matrix3)\n",
    "    ctr1, ctr2, ctr3 = 0, 0, 0\n",
    "    for i in range(len(test_target1)):\n",
    "        if(predicted_val1[i] == test_target1[i]):\n",
    "            ctr1 += 1\n",
    "        if(predicted_val2[i] == test_target2[i]):\n",
    "            ctr2 += 1\n",
    "    for i in range(len(test_target3)):\n",
    "        if(predicted_val3[i] == test_target3[i]):\n",
    "            ctr3 += 1\n",
    "    print(\"Accuracy for MNIST test data on svm rbf gamma(default) = \"+str((ctr1/len(test_target1))*100))\n",
    "    actual1 = pd.Series(test_target1, name='tt1')\n",
    "    pred1 = pd.Series(predicted_val1, name='pd1')\n",
    "    print(\"Confusion Matrix for test data\")\n",
    "    print(pd.crosstab(actual1, pred1))\n",
    "    print(\"Accuracy for MNIST validation data on svm rbf gamma(default) = \"+str((ctr2/len(test_target2))*100))\n",
    "    actual2 = pd.Series(test_target2, name='tt2')\n",
    "    pred2 = pd.Series(predicted_val2, name='pd2')\n",
    "    print(\"Confusion Matrix for validation data\")\n",
    "    print(pd.crosstab(actual2, pred2))\n",
    "    print(\"Accuracy for USPS data on svm rbf gamma(default) = \"+str((ctr3/len(test_target3))*100))\n",
    "    actual3 = pd.Series(test_target3, name='tt3')\n",
    "    pred3 = pd.Series(predicted_val3, name='pd3')\n",
    "    print(\"Confusion Matrix for USPS data\")\n",
    "    print(pd.crosstab(actual3, pred3))\n",
    "    return predicted_val1\n",
    "    \n",
    "SVM_predicted = rbf_classification_default_gamma(training_matrix, training_target, \n",
    "                                    test_matrix, test_target, \n",
    "                                    validation_matrix, validation_target, \n",
    "                                    np.asarray(USPSMat), np.asarray(USPSTar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST test data on svm rbf gamma(1) = 17.59\n",
      "Accuracy for MNIST validation data on svm rbf gamma(1) = 18.240000000000002\n",
      "Accuracy for USPS data on svm rbf gamma(1) = 10.000500025001251\n"
     ]
    }
   ],
   "source": [
    "#RBF SVM with gamma 1\n",
    "def rbf_classification_default_gamma(data, target, test_matrix1, test_target1, \n",
    "                                        test_matrix2, test_target2, \n",
    "                                        test_matrix3, test_target3):\n",
    "    classification = svm.SVC(kernel='rbf', gamma=1) #gamma set to 1 with rbf kernel. takes whole night to run and gives sad accuracy\n",
    "    classification.fit(data, target)\n",
    "    predicted_val1 = classification.predict(test_matrix1)\n",
    "    predicted_val2 = classification.predict(test_matrix2)\n",
    "    predicted_val3 = classification.predict(test_matrix3)\n",
    "    ctr1, ctr2, ctr3 = 0, 0, 0\n",
    "    for i in range(len(test_target1)):\n",
    "        if(predicted_val1[i] == test_target1[i]):\n",
    "            ctr1 += 1\n",
    "        if(predicted_val2[i] == test_target2[i]):\n",
    "            ctr2 += 1\n",
    "    for i in range(len(test_target3)):\n",
    "        if(predicted_val3[i] == test_target3[i]):\n",
    "            ctr3 += 1\n",
    "    print(\"Accuracy for MNIST test data on svm rbf gamma(1) = \"+str((ctr1/len(test_target1))*100))\n",
    "    actual1 = pd.Series(test_target1, name='tt1')\n",
    "    pred1 = pd.Series(predicted_val1, name='pd1')\n",
    "    print(\"Confusion Matrix for test data\")\n",
    "    print(pd.crosstab(actual1, pred1))\n",
    "    print(\"Accuracy for MNIST validation data on svm rbf gamma(1) = \"+str((ctr2/len(test_target2))*100))\n",
    "    actual2 = pd.Series(test_target2, name='tt2')\n",
    "    pred2 = pd.Series(predicted_val2, name='pd2')\n",
    "    print(\"Confusion Matrix for validation data\")\n",
    "    print(pd.crosstab(actual2, pred2))\n",
    "    print(\"Accuracy for USPS data on svm rbf gamma(1) = \"+str((ctr3/len(test_target3))*100))\n",
    "    actual3 = pd.Series(test_target3, name='tt3')\n",
    "    pred3 = pd.Series(predicted_val3, name='pd3')\n",
    "    print(\"Confusion Matrix for USPS data\")\n",
    "    print(pd.crosstab(actual3, pred3))\n",
    "    \n",
    "rbf_classification_default_gamma(training_matrix, training_target, \n",
    "                                    test_matrix, test_target, \n",
    "                                    validation_matrix, validation_target, \n",
    "                                    np.asarray(USPSMat), np.asarray(USPSTar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST test data on random forest = 97.06\n",
      "Confusion Matrix for test data\n",
      "pd1    0     1     2    3    4    5    6    7    8    9\n",
      "tt1                                                    \n",
      "0    969     0     0    0    0    2    3    1    4    1\n",
      "1      0  1123     3    3    0    2    2    0    1    1\n",
      "2      6     0  1000    5    3    0    4    8    6    0\n",
      "3      0     0     8  976    0    7    0    9    8    2\n",
      "4      1     0     1    0  956    0    6    0    2   16\n",
      "5      3     0     0   11    3  860    6    1    5    3\n",
      "6      5     3     0    0    2    4  941    0    3    0\n",
      "7      1     3    18    1    1    0    0  992    2   10\n",
      "8      3     0     6    9    5    5    3    4  929   10\n",
      "9      7     5     1   11   12    2    1    4    6  960\n",
      "Accuracy for MNIST validation data on random forest = 97.5\n",
      "Confusion Matrix for validation data\n",
      "pd2    0     1    2     3    4    5    6     7    8    9\n",
      "tt2                                                     \n",
      "0    980     0    3     0    0    0    2     0    4    2\n",
      "1      0  1053    5     1    1    1    0     0    2    1\n",
      "2      3     0  968     0    2    1    1     6    6    3\n",
      "3      1     0    4  1001    0    8    0     4    8    4\n",
      "4      0     4    0     0  961    0    2     1    1   14\n",
      "5      2     0    4    12    2  878   11     1    3    2\n",
      "6      1     0    0     0    1    1  960     0    4    0\n",
      "7      1     5    9     1    3    0    0  1062    0    9\n",
      "8      1     5    4     5    1    6    4     1  975    7\n",
      "9      5     3    2     9   10    4    0     7    9  912\n",
      "Accuracy for USPS data on svm random forest = 40.7420371018551\n",
      "Confusion Matrix for USPS data\n",
      "pd3    0    1     2     3     4     5    6    7    8    9\n",
      "tt3                                                      \n",
      "0    653   14   275    56   448   153   55  106    2  238\n",
      "1     46  560   120   108    52    96   19  984   14    1\n",
      "2     89   28  1281    72    48   196   13  265    5    2\n",
      "3     38    6    91  1305    53   316    2  170    4   15\n",
      "4     13  199    53    20  1088   185   15  389   21   17\n",
      "5    145   32   123    74    25  1459   19  115    5    3\n",
      "6    291   44   208    24    88   356  842  137    2    8\n",
      "7     39  337   356   252    37   253   32  684    2    8\n",
      "8     37   46   141   207   100  1123   62  100  170   14\n",
      "9     15  260   225   318   237   128    6  621   84  106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def rf_classification(data, target, test_matrix1, test_target1, \n",
    "                      test_matrix2, test_target2, \n",
    "                      test_matrix3, test_target3):\n",
    "    classification = RandomForestClassifier(n_estimators=1000, max_depth=60, random_state=0) #1000 decision trees with each tree of depth upto 60\n",
    "    classification.fit(data, target)\n",
    "    predicted_val1 = classification.predict(test_matrix1)\n",
    "    predicted_val2 = classification.predict(test_matrix2)\n",
    "    predicted_val3 = classification.predict(test_matrix3)\n",
    "    ctr1, ctr2, ctr3 = 0, 0, 0\n",
    "    for i in range(len(test_target1)):\n",
    "        if(predicted_val1[i] == test_target1[i]):\n",
    "            ctr1 += 1\n",
    "        if(predicted_val2[i] == test_target2[i]):\n",
    "            ctr2 += 1\n",
    "    for i in range(len(test_target3)):\n",
    "        if(predicted_val3[i] == test_target3[i]):\n",
    "            ctr3 += 1\n",
    "    print(\"Accuracy for MNIST test data on random forest = \"+str((ctr1/len(test_target1))*100))\n",
    "    actual1 = pd.Series(test_target1, name='tt1')\n",
    "    pred1 = pd.Series(predicted_val1, name='pd1')\n",
    "    print(\"Confusion Matrix for test data\")\n",
    "    print(pd.crosstab(actual1, pred1))\n",
    "    print(\"Accuracy for MNIST validation data on random forest = \"+str((ctr2/len(test_target2))*100))\n",
    "    actual2 = pd.Series(test_target2, name='tt2')\n",
    "    pred2 = pd.Series(predicted_val2, name='pd2')\n",
    "    print(\"Confusion Matrix for validation data\")\n",
    "    print(pd.crosstab(actual2, pred2))\n",
    "    print(\"Accuracy for USPS data on svm random forest = \"+str((ctr3/len(test_target3))*100))\n",
    "    actual3 = pd.Series(test_target3, name='tt3')\n",
    "    pred3 = pd.Series(predicted_val3, name='pd3')\n",
    "    print(\"Confusion Matrix for USPS data\")\n",
    "    print(pd.crosstab(actual3, pred3))\n",
    "    return predicted_val1\n",
    "    \n",
    "RF_predicted = rf_classification(training_matrix, training_target, \n",
    "                                    test_matrix, test_target, \n",
    "                                    validation_matrix, validation_target, \n",
    "                                    np.asarray(USPSMat), np.asarray(USPSTar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of majority voting = 95.89999999999999%\n"
     ]
    }
   ],
   "source": [
    "majority_voted = []\n",
    "for i in range(len(LR_predicted)):\n",
    "    a = [] \n",
    "    a.append(LR_predicted[i])\n",
    "    a.append(NN_predicted[i])\n",
    "    a.append(RF_predicted[i])\n",
    "    a.append(SVM_predicted[i]) #predicted labels from all the classifiers\n",
    "    majority_voted.append(max(a,key=a.count)) #pick predicted class with most number of occurences in the list\n",
    "ctr = 0\n",
    "for i in range(len(majority_voted)):\n",
    "    if(majority_voted[i] == test_target[i]):\n",
    "        ctr += 1 \n",
    "print(\"Result of majority voting = \" + str((ctr/len(majority_voted))*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
